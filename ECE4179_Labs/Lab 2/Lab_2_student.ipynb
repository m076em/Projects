{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\"> *This notebook is best viewed in jupyter lab/notebook. You may also choose to use Google Colab but some parts of the images/colouring will not be rendered properly.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR2ekMPyECb6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# <p style=\"text-align: center;\">Lab 2 (Weeks 3,4): Model training with Linear Regression and Logistic Regression</p>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\" width=\"200\" height=\"200\" />\n",
    "\n",
    "<!-- ![linear-vs-logistic-regression--medium](https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg) -->\n",
    "\n",
    "Welcome to your second lab! This notebook contains all the code and comments that you will need to submit. Labs run over two weeks and the places where you need to edit are highlighted in red. Please note that the colour highlighting might not work across all IDEs, so make sure you check and run all cells! </br>\n",
    "\n",
    "This Lab has three tasks. The first two tasks test your basic knowledge regarding the linear and logistic regression and their applcations. In the last task, you will analyse the results.\n",
    "- <b>Task 1:</b> Simple linear regression\n",
    "- <b>Task 2:</b> Logistic regression and gradient descent\n",
    "- <b>Task 3:</b> Analysing convergence and accuracy\n",
    "    \n",
    "Each sub task will contain code to complete, and/or a worded question, so ensure you complete everything before submitting.\n",
    "\n",
    "Feel free to add in your own markdown for additional comments, and also directly comment your code.\n",
    "\n",
    "__Submission details:__\n",
    "- __Make sure you have run all your cells from top to bottom (you can click _Kernel_ and _Restart Kernel and Run All Cells_).__ </br>\n",
    "- __Submit the Jupyter Notebook (_Lab_2_student.ipynb_).__\n",
    "- __Outputs must be visible upon submission. We will also be re-running your code__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Enter you credentials below</b>\n",
    "\n",
    "- <b>Student Name:</b> Firstname Lastname\n",
    "- <b>Student ID:</b> 123456789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Task 1: Simple linear regression](#simple-linear-regression)    \n",
    "    * [1.1 Simple linear regression formulation](#lr-formulation)\n",
    "    * [1.2 Test and visualise the linear regression](#test-lr)\n",
    "    \n",
    "    \n",
    "* [Task 2: Logistic Regression and Gradient Descent](#logistic-gd)\n",
    "    * [2.1 The sigmoid function](#sigmoid)\n",
    "    * [2.2 Predicting class probabilities via logistic regression](#predict)\n",
    "    * [2.3 Training a model via Gradient Descent](#train)\n",
    "    * [2.4 Evaluating the trained model](#evaluate)\n",
    "    \n",
    "\n",
    "* [Task 3: Analysing convergence and accuracy](#analyse-convergence-and-accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1689299242807,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "_oOV5kbmEUhq"
   },
   "outputs": [],
   "source": [
    "## Libraries, you do not need to import any additional libraries for this lab\n",
    "\n",
    "import numpy as np ## Numpy is the fundamental building block of understanding tensor (matrices) within Python\n",
    "import matplotlib.pyplot as plt ## Matplotlib.pyplot is the graphing library that we will be using throughout the semester\n",
    "import random ## Useful for sampling\n",
    "\n",
    "import os ## Useful for running command line within python\n",
    "from IPython.display import Image ## For markdown purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpz-glOgEfhm"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# Before you begin\n",
    "\n",
    "We have provided some numerical answers for you to aim for. To replicate these results, do not change any of the codes that are labelled \"Do not change\".\n",
    "\n",
    "Throughout this lab, there will be code and written answers that you need to fill in / complete. The comments in the code snippet and markdown text will guide you on what you need to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCA0XOeFGJCt"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 1 - Simple linear regression <a class=\"anchor\" id=\"simple-linear-regression\"></a>\n",
    "\n",
    "In this section, you will be writing the first parts of your code that is essential to predict the outcome of a linear regression problem. </br>\n",
    "In detail, you are going to\n",
    "- 1.1 Implement a simple linear regression function\n",
    "- 1.2 Write code to visualise the line of regression using the trained linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Learning Objective \n",
    "\n",
    "This task aims to understand and implement simple linear regression using the Least Squares Regression method to find the best line fit for given samples and visually analyze its performance. By the end of this task, you will be able to:\n",
    "1. Formulate a simple linear regression equation $y = wx + b$.\n",
    "2. Implement the Least Squares Regression method to find the estimated slope $\\hat{m}$ and intercept $\\hat{b}$ of the regression line, minimizing the square distances from each sample to the line.\n",
    "3. Apply the developed simple linear regression function to samples of different distributions.\n",
    "4. Visualize and analyze the performance of the linear regression model on the given samples to understand how well it fits the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pHwx0_pExd-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Linear Regression - A quick recap\n",
    "\n",
    "Linear regression analysis is used to predict the value of a (dependent) variable based on the value of another (independent) variable assuming the there is a linear relationship between the two. Simply speaking, linear regression aims to find the best line fit for the given data. (See figure below)\n",
    "\n",
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-in-machine-learning.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoJw07FxMLpn"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.1 Simple linear regression formulation <a class=\"anchor\" id=\"lr-formulation\"></a>\n",
    "\n",
    "Let's consider a simple linear regression problem with one independent variable $x$ and one dependent variable $y$. The simple linear regression equation we will use is written below.\n",
    "\n",
    "$$y=wx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7n9NJKTRY0x"
   },
   "source": [
    "We are going to use the **Least Squares Regression** to solve the problem. For a given set of data $(X,Y)$, where $X = \\{x_1, x_2, ... x_n\\}$ and $Y = \\{y_1, y_2, ... y_n\\}$, the equations are given:\n",
    "\n",
    "To estimate the slope $w$ of the regression line:\n",
    "\n",
    "$$\\hat{w}=\\frac{\\sum^{n}_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum^{n}_{i=1}(x_i-\\bar{x})^{2}}$$\n",
    "\n",
    "To estimate the intercept $b$ of the regression line:\n",
    "\n",
    "$$\\hat{b}=\\bar{y}-\\hat{w}\\bar{x}$$\n",
    "\n",
    "Here, $\\bar{x}$ and $\\bar{y}$ are the mean values of the data points in $X$ and $Y$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOxMHnQnSGli"
   },
   "source": [
    "### Write a simple linear regression function\n",
    "\n",
    "In this task, you are now first asked to write a function that takes $n$ samples in the form of $(x_i,y_i)$ as inputs and computes the slope $w$ and intercept $b$ of the regression line. (Assuming both $x$ and $y$ are 1D)</br>\n",
    "\n",
    "\n",
    "The input to your function is denoted X and Y, where X is a 1D array of size $n$ and Y is a 1D array of size $n$ as well. The output of your function is denoted $w$ and $b$, where $w$ is the slope of the regression line and $b$ is the intercept of the regression line. \n",
    "\n",
    "_Hint_: Use the _numpy_ library you have been introduced to in the previous lab to allow easy computation of multi-dimensional input values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1689299242807,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "j29qM4tWGRA2",
    "outputId": "d2ba06b3-fd1c-4873-f037-05157076fafb"
   },
   "outputs": [],
   "source": [
    "## Implement the linear regression function (Assuming both X and Y are 1d)\n",
    "def linear_regression(X, Y):\n",
    "    # The shape of X is (n,), where n is the number of samples\n",
    "    # The shape of Y is (n,), where n is the number of samples\n",
    "\n",
    "    w = ???\n",
    "    b = ???\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxlrXwPSTxcV"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 1.2 Test and visualise the linear regression <a class=\"anchor\" id=\"test-lr\"></a>\n",
    "\n",
    "With the function you wrote in section 1.1, let's test your linear regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PedwTE9UXICp"
   },
   "source": [
    "**(a) We first load and visualise the samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1689299243561,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "sOXcWHAYT9eN",
    "outputId": "ce9bde97-edd5-49ed-e61f-831b976b2e7b"
   },
   "outputs": [],
   "source": [
    "# Load numpy dataset from lab2_task1_sample1.npz\n",
    "# Hint: use np.load()\n",
    "loaded_sample1 = ???\n",
    "\n",
    "# Create train and test datasets\n",
    "# Data was saved in a dictionary-liked form, where the keys are ('arr_0','arr_1')\n",
    "# Load the values into correct variables according to the following mapping.\n",
    "# (X1 <-- arr_0, Y1 <-- arr_1)\n",
    "X1 = loaded_sample1['arr_0']\n",
    "Y1 = loaded_sample1['arr_1']\n",
    "\n",
    "# Show samples as a scatter plot\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p-AgaN9Xffj"
   },
   "source": [
    "**(b) Now find parameters of the regression line with the given samples and your linear regression function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1689299243562,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "SQojGBFgVXdh",
    "outputId": "9bb88f17-1383-4e37-a85b-ebf5513abf1d"
   },
   "outputs": [],
   "source": [
    "## Find the slope and intercept of the regression line using your linear regression function\n",
    "w1, b1 = ???\n",
    "print(\"w = {:.2f} b = {:.2f}\".format(w1, b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c11JpHdDYDDW"
   },
   "source": [
    "To test the correctness of your implementation, the values you get should be close to\n",
    "$w=1.16$, $b=0.40$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbFPqt7kZix4"
   },
   "source": [
    "**(c) Use your experience with numpy and matplotlib from the previous lab to visualise the regression line in the range $x \\in [0, 4]$.** </br>\n",
    "In detail, we ask you to:\n",
    "- Visualise the regression line as a **line plot in <font color='red'>red</font> colour**.\n",
    "- Visualise the samples as a **scatter plot in <font color='blue'>blue</font> colour**.\n",
    "\n",
    "Please plot both into the same figure! </br>\n",
    "Use **50 data points** on the line to make it smooth, and make sure to add an appropriate **plot title** and to **label the axes**!</br>\n",
    "\n",
    "_Hint_: Check the matplotlib document for details how to do this. You can find many examples there for a variety of different applications. (Check the lab instructions .pdf to see an example of what is expected here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1689299244249,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "BStxOuH1YNdP",
    "outputId": "e361a22a-a873-42d2-9b6d-b398e539ccac"
   },
   "outputs": [],
   "source": [
    "## Viuslise the regression line with given samples (both on the same plot)\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQBxnWJldXcw"
   },
   "source": [
    "**(d) Now let's try it on a different set of samples. Load the new samples and visualise them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1689299244923,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "VYPmhbTacGUI",
    "outputId": "8632f936-f26b-44a5-bb2c-61e09d1341e1"
   },
   "outputs": [],
   "source": [
    "# Load numpy dataset from lab2_task1_sample2.npz\n",
    "# Hit: use np.load()\n",
    "loaded_sample2 = ???\n",
    "\n",
    "# Create train and test datasets\n",
    "# Data was saved in a dictionary-liked form, where the keys are ('arr_0','arr_1')\n",
    "# Load the values into correct variables according to the following mapping.\n",
    "# (X2 <-- arr_0, Y2 <-- arr_1)\n",
    "X2 = loaded_sample2['arr_0']\n",
    "Y2 = loaded_sample2['arr_1']\n",
    "\n",
    "# Show samples as a scatter plot\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJMYwwULetQn"
   },
   "source": [
    "**(e) Use your linear regression function to fit a regression line for the new samples (X2, Y2). And visualise the result.**\n",
    "\n",
    "In detail, we ask you to:\n",
    "- Visualise the new regression line as a **line plot in <font color='red'>red</font> colour**.\n",
    "- Visualise the new samples as a **scatter plot in <font color='blue'>blue</font> colour**.\n",
    "\n",
    "Please plot both into the same figure! </br>\n",
    "\n",
    "Use **50 data points** to get a smooth line, and make sure to add an appropriate **plot title** and to **label the axes**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1689299244924,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "8wu02OHJec8I",
    "outputId": "fedab2bf-9462-4788-808b-414c2848a8f9"
   },
   "outputs": [],
   "source": [
    "## Find the slope and intercept of the regression line using your linear regression function\n",
    "w2, b2 = ???\n",
    "print(\"w = {:.2f} b = {:.2f}\".format(w2, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSZFutTdOB-w"
   },
   "source": [
    "To test the correctness of your implementation, the values you get should be close to\n",
    "$w=0.38$, $b=-0.21$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1689299245403,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "p66btixPfcO7",
    "outputId": "0b38f393-873b-4050-ab01-e1d9ce9a3a44"
   },
   "outputs": [],
   "source": [
    "## Viuslise the regression line with given samples (both on the same plot)\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1689299245719,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "BqfLTRY_fnBN",
    "outputId": "8acc27c2-4f09-48c9-c092-c5c0b4723ccc"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Questions: \n",
    "\n",
    "1. Does linear regression give a good fit for the new samples (X2, Y2)? Why or why not?\n",
    "    \n",
    "2. What's the difference between two sets of samples (Sample 1 and Sample 2)?\n",
    "\n",
    "### Answer\n",
    "    \n",
    "1. ???\n",
    "    \n",
    "2. ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K9lmwbBgNXy"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "# Task 2 - Logistic Regression and Gradient Descent <a class=\"anchor\" id=\"logistic-gd\"></a>\n",
    "\n",
    "In this section, you will be writing the first parts of your code that is essential to predict the outcome of a logistic regression problem. </br>\n",
    "In detail, you are going to\n",
    "\n",
    "- 2.1 Implement and visualise the **sigmoid function**\n",
    "- 2.2 Write code to **predict the outcome** of a classification problem using a pre-trained logistic regression model\n",
    "- 2.3 Train a model via Gradient Descent\n",
    "- 2.4 Evaluate the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Learning Objective \n",
    "\n",
    "This task aims to equip you with the knowledge and practical skills required to implement logistic regression, understand gradient descent as an optimization technique, and evaluate the performance of their trained logistic regression model on training and test datasets. By the end of this lab task, you will be able to:\n",
    "1. Implement and visualize the sigmoid function to map regression outputs into a range from 0 to 1, necessary for logistic regression.\n",
    "2. Use the sigmoid function to predict the class probabilities of a classification problem using logistic regression with a pre-trained model.\n",
    "3. Write code to perform gradient descent, compute gradients, and the cost of the logistic regression model to train it on given training data.\n",
    "4. Train their own logistic regression model using gradient descent with a fixed number of iterations.\n",
    "5. Evaluate the trained logistic regression model on previously unseen test data points.\n",
    "6. Convert class probabilities to actual predicted labels and calculate the accuracy of the model for both training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiL_9HC4B2gr"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.1  The sigmoid function <a class=\"anchor\" id=\"sigmoid\"></a>\n",
    "\n",
    "The '_sigmoid function_' $\\sigma$, sometimes also called '_logistic function_', is a mathematical function that shows a characteristic \"S\"-shaped curve as you've seen during the lecture (hence its name!). We commonly use this function in our logistic regression to map the regression outputs to a range from 0 to 1. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYtVhfyqB4Wc"
   },
   "source": [
    "### Computing the sigmoid\n",
    "In this task, you are now first asked to write a function that computes the output of the sigmoid function $\\sigma(\\boldsymbol{x})$ for any input value $\\boldsymbol{x}$. </br>\n",
    "\n",
    "_Hint:_ Use the _numpy_ library you have been introduced to in the previous lab to allow easy computation of multi-dimensional input values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1689299245720,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "cdtC2PlsgMMC",
    "outputId": "9f4f3fd3-5866-4963-e76c-d4ed1e0b0bed"
   },
   "outputs": [],
   "source": [
    "# Implement the sigmoid function\n",
    "def sigmoid(x):\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Aj05d2bB_6x"
   },
   "source": [
    "You will now use your experience with numpy and matplotlib from the previous lab to visualise the output range of our implemented sigmoid function for a 1 dimensional case in the range $x \\in [-10, 10]$. </br>\n",
    "In detail, we ask you to:\n",
    "- Visualise the outputs of the sigmoid as a **line plot in <font color='blue'>blue</font> colour**.\n",
    "- Visualise the outputs of the sigmoid as a **scatter plot in <font color='red'>red</font> colour**.\n",
    "\n",
    "Please plot both into the same figure! </br>\n",
    "Use **50 data points** to get a smooth plot, and make sure to add an appropriate **plot title** and to **label the axes**!</br>\n",
    "\n",
    "_Hint_: Check the matplotlib docu for details how to do this. You can find many examples there for a variety of different applications. (Check the lab instructions .pdf to see an example of what is expected here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1689299246634,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "jW_cMu15CFol",
    "outputId": "e236c541-75d2-4073-88da-39f996f4d0c4"
   },
   "outputs": [],
   "source": [
    "## Visualise the output of the sigmoid function in a range from -10 to 10\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1689299246634,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "l7oPFJiWCP95",
    "outputId": "ba1bd0b0-7d66-4542-9459-e7694f9c9041"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Discussion\n",
    "\n",
    "1. What is the difference between the line plot and the scatter plot?\n",
    "2. What would happen if you only used a small number of datapoints to plot?\n",
    "\n",
    "### Answer\n",
    "1. ???\n",
    "2. ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtTRRt2ICScY"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.2 Predicting class probabilities via logistic regression <a class=\"anchor\" id=\"predict\"></a>\n",
    "\n",
    "You will now use your implemented sigmoid function to solve an actual classification problem using logistic regression. </br>\n",
    "As discussed in the lecture, a prediction $\\hat{y}$ can be obtained by using our logistic regression model via $\\hat{y}=\\sigma(\\boldsymbol{w}^\\top \\boldsymbol{x})$\n",
    "\n",
    "Note that for this example, we want to be able to use many samples at the same time - all of which are stored in one single vector $X$, which is similar to the test case $\\boldsymbol{x_3}$ from before.\n",
    "\n",
    "Also note that we predict the distribution over the classes, _i.e._ the probablity for each class -> to get the 'hard' class label, we will later on assign everything below the probablity of 0.5 to class1 and any probability above to class2 (boundary could be included in either)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiLJWpQ5CU8I"
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1689299246635,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "ib1HhdQzCYTg",
    "outputId": "7b3e5a53-79ac-49ce-90ee-c42a526fced0"
   },
   "outputs": [],
   "source": [
    "# Read in the lab2_task2_data.npz using numpy --> data has been saved via np.savez (check docu for more details)\n",
    "\n",
    "# Components can be accessed like a dictionary after the file has been loaded, and the file contains the following:\n",
    "# 'X_train' : training data we're going to use\n",
    "# 'y_train' : labels for the training data\n",
    "# 'X_test'  : test data we're going to use for evaluation, but NOT for training\n",
    "# 'y_test'  : labels for the test data\n",
    "# 'w_pret'  : a set of pretrained weights for the logistic regression model\n",
    "\n",
    "loaded_data = ???\n",
    "X_train = loaded_data['X_train']\n",
    "y_train = loaded_data['y_train']\n",
    "X_test = loaded_data['X_test']\n",
    "y_test = loaded_data['y_test']\n",
    "w_pret = loaded_data['w_pret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1608,
     "status": "ok",
     "timestamp": 1689299248229,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "S1bze5V-CaL0",
    "outputId": "d24dbc72-ce23-43a9-8f53-3c80d1b059e6"
   },
   "outputs": [],
   "source": [
    "# Add side by side plots here to visualise your train and test data (Use subplots).\n",
    "# These are the two classes that you will be classifying.\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1689299248230,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "_rLQrbvlCb7j",
    "outputId": "32d5af05-1a4f-4144-8bfb-8272e3c2fe9e"
   },
   "outputs": [],
   "source": [
    "## Check the shape of the data!\n",
    "# Note that we assume certain shapes of data for the basic logistic regression formulas to work,\n",
    "# so make sure you understand which elements should be multiplied with each other!\n",
    "# Hint: In case the data is stored in a different shape, you can easily transpose the matrices!\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_test: {y_test.shape}')\n",
    "print(f'w_pret: {w_pret.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPUTpqb8ChZ0"
   },
   "source": [
    "### Implement a logistic regression model\n",
    "\n",
    "You are now going to\n",
    "- Implement a function to predict outcomes using a logistic regression model (taking in data $\\boldsymbol{X}$ and parameters $\\boldsymbol{w}$)\n",
    "- Test your function on the example data provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1689299248230,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "iNmlP7VxCjzn",
    "outputId": "8a07b4e9-ed9d-41b1-c76b-6f3138c6e130"
   },
   "outputs": [],
   "source": [
    "# Write a prediction function -> We predict the output class probability, NOT the class label (no 0,1 rounding)\n",
    "def predict(X, w):\n",
    "\n",
    "    # Input argument(s):\n",
    "    # X - the input data\n",
    "    # w - the weights from the trained model\n",
    "     \n",
    "        \n",
    "    # Output:\n",
    "    # Probability prediction of each data point (y_hat)\n",
    "\n",
    "    # Reshape X input to have data in the columns [mxn]-->[nxm]\n",
    "    ???\n",
    "\n",
    "    # Perform Matrix multiplication between the inputs and the weights\n",
    "    ???\n",
    "\n",
    "    # Calling Sigmoid\n",
    "    y_hat = ???\n",
    "\n",
    "    return ??? #output y_hat as an mx1 array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPUmWfxJCmrl"
   },
   "source": [
    "Test your prediction function using the following toy data points / samples:\n",
    "- $\\boldsymbol{X}_1 = [[0.5, 0.1]]$\n",
    "- $\\boldsymbol{X}_2 = [ [-0.5, -0.7], [0.4, 0.2] ]$\n",
    "- $\\boldsymbol{X}_3 = [ [-0.3, -0.15], [0.89, -0.02], [-0.35, 0.01], [0.26, -0.64] ]$\n",
    "\n",
    "Note that our data is stored as [number of samples, dim], so you need to pay attention to possibly required transpose operations to perform the calculations correctly. </br>\n",
    "We also want the same to be true for our predictions, _i.e._ return them in the shape of [number of samples, 1] to match the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1689299248230,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "v5yTmdhrCpH2",
    "outputId": "0059cd42-d3b9-412f-a545-d8b8ed11af76"
   },
   "outputs": [],
   "source": [
    "## Define the toy input data\n",
    "X_1 = ???\n",
    "X_2 = ???\n",
    "X_3 = ???\n",
    "\n",
    "# print(w_pret.shape)\n",
    "## Obtain predictions using predict function and pretrained parameters w_pret\n",
    "y_hat_1 = predict(X_1, w_pret)\n",
    "y_hat_2 = predict(X_2, w_pret)\n",
    "y_hat_3 = predict(X_3, w_pret)\n",
    "\n",
    "print(f'y_hat_1: {y_hat_1}')\n",
    "print(f'y_hat_2: {y_hat_2}')\n",
    "print(f'y_hat_3: {y_hat_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK8S2ox1CrdW"
   },
   "source": [
    "**If your predict function works as intended, your results should be close to:** </br>\n",
    "y_hat_1: &nbsp;[[0.51370692]]</br>\n",
    "y_hat_2: [[0.61837619]</br>\n",
    "&emsp;&emsp;&emsp;&emsp; &nbsp;&nbsp;[0.48409849]]</br>\n",
    "y_hat_3: [[0.51192789]</br>\n",
    "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.56831845]</br>\n",
    "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.47251372]</br>\n",
    "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.65665833]]</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FM1xYa1Cuqz"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.3 Training a model via Gradient Descent <a class=\"anchor\" id=\"train\"></a>\n",
    "    \n",
    "In this task, you will be writing code for the essential components to **train your own logistic model via Gradient Descent** given some training data. </br>\n",
    "In detail, you are going to\n",
    "- Implement a function that computes and returns **gradient and cost** of the logistic regression\n",
    "- Write code to perform the actual **gradient descent algorithm** for a fixed number of iterations and **train your own logistic regression model** given some training data\n",
    "- **Evaluate your model** on previously unseen test data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5usnQq7CzFG"
   },
   "source": [
    "As discussed in more detail during the lecture, we commonly use the so-called _Cross Entropy_ Loss to calculate the cost of our logistic regression problem. This loss function can be defined as </br>\n",
    "</br>\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{\\mathrm{CE}}(\\boldsymbol{w})= - \\frac{1}{m}\\sum_{i=1}^{m} \\Big\\lbrace y_i \\log \\Big(\\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i}\\Big) + \\left( 1 - y_i \\right) \\log \\Big( 1- \\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i}\\Big) \\Big\\rbrace\n",
    "\\end{equation}\n",
    "</br>\n",
    "In this notation, $\\sigma(z) = 1 / (1 + \\exp(-z))$ denotes the **sigmoid** function, and $(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2),\\dots,(\\boldsymbol{x}_m,y_m)$ with $\\boldsymbol{x}_i \\in \\mathbb{R}^n, y_i \\in \\lbrace 0, 1\\rbrace$ represent the $m$ training samples (with labels $y_i$). </br>\n",
    "The gradient of the cross entropy loss w.r.t. the weights $\\boldsymbol{w}$ can be written as\n",
    "</br>\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}} = \\frac{1}{m}\\sum_{i=1}^{m} \\Big(\\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i} - y_i \\Big) \\boldsymbol{x}_i\n",
    "\\end{equation}\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOrtiqO2C19Q"
   },
   "source": [
    "###  Gradient and Cost Computation\n",
    "In this part, we want to define a function that is able to compute our cross-entropy loss $\\mathcal{L}_{\\mathrm{CE}}$, as well as the gradient $\\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}}$ of our loss $\\mathcal{L}_{\\mathrm{CE}}$ _w.r.t._ the parameters $\\boldsymbol{w}$. </br>\n",
    "\n",
    "As you can see above, all we need to compute the gradient vector is the prediction of the model $\\hat{y}$ and the actual labels $y$, as well as the input data points $\\boldsymbol{X}$. The loss itself is even more simple and only requires the predictions $\\hat{y}$ and true labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1689299248231,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "0Tnuu0HoC4S6",
    "outputId": "f5b600a0-98ff-4b55-b3ff-513193a7156c"
   },
   "outputs": [],
   "source": [
    "def compute_loss_and_grad(X, y, y_hat):\n",
    "    # Inputs:\n",
    "    #    X - Set of samples (each sample is a row in X),\n",
    "    #    y - Corresponding ground-truth labels\n",
    "    #    y_hat - Predicted class probabilities\n",
    "\n",
    "    # Import smallest number represented to handle log(0) edge case\n",
    "    eps = 1e-12\n",
    "\n",
    "    # Compute the mean cross-entropy loss w.r.t. the parameters w (mean as defined in lecture)\n",
    "    # log(0) might throw error, so handled via small eps\n",
    "    loss = ???\n",
    "\n",
    "    # Compute the gradient vector (mean over all samples as defined in lecture)\n",
    "    grad_vec = ???\n",
    "\n",
    "    # Return loss and gradient vector\n",
    "    return loss, grad_vec.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSJA2hFeC7Y6"
   },
   "source": [
    "### Training with Gradient Descent\n",
    "\n",
    "Gradient descent, sometimes also referred to as _steepest descent_, is a popular first-order iterative optimisation method that has become ubiquitous in the machine and deep learning context. As you have heard in the lecture, the idea is to find the local minimum of a differentiable function by repeatedly taking steps in the opposite direction of the gradient of the function at the current point - i.e. in the direction of its steepest descent.\n",
    "\n",
    "In this section of the lab, you are going to implement the **Gradient Descent algorithm** as a function that we can use afterwards to train our logistic regression model!\n",
    "\n",
    "The main parts of the algorithm work as follows:\n",
    "- Initialise hyperparameters like step-size aka learning rate, and number of iterations\n",
    "- Randomly initialise the set of parameters $\\boldsymbol{w}_{init}$ that shall be optimised\n",
    "- For a certain number of iterations, do:\n",
    "    - Obtain the prediction using the current weights $\\boldsymbol{w}_i$ and training data $\\boldsymbol{X}_{train}$\n",
    "    - Compute the loss $\\mathcal{L}_{\\mathrm{CE}}$ and the gradient vector $\\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}}$ w.r.t. the current parameters $\\boldsymbol{w}_i$\n",
    "    - Update the parameters using the gradient vector and learning rate _lr_\n",
    "- After all iterations are finished, return the final optimised set of parameters\n",
    "\n",
    "In addition, we ask you to also:\n",
    "- Return a list of all losses (one value for each iteration)\n",
    "- Return a list of all gradient vectors (one vector for each iteration)\n",
    "- Implement an option via the argument \"logging\" to switch on printing a string containing the 'iteration' and the 'loss' for each iteration\n",
    "\n",
    "Note that the initial set of parameters $\\boldsymbol{w}_{init}$, the hyperparameters as well as the training data $\\boldsymbol{X}_{train}$ and labels $\\boldsymbol{y}_{train}$ are passed as input arguments to your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1689299248231,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "WKL06JjFC9fA"
   },
   "outputs": [],
   "source": [
    "## Setting some hyperparameters: (Do not change these settings!)\n",
    "lr = 0.5         # Learning rate\n",
    "num_epochs = 20    # Number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1689299248231,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "J_EwI9XOC_Jy",
    "outputId": "af51400e-84a8-4092-9bcb-1e8dbb617151"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(w_init, num_epochs, lr, X_train, y_train, logging=False):\n",
    "    ## Create empty lists to store the values for loss and gradient vector over all\n",
    "    #  'num_epochs' iterations of our gradient descent optimisation procedure\n",
    "    losses = []\n",
    "    grad_vecs = []\n",
    "\n",
    "    # Init the parameters\n",
    "    w = w_init\n",
    "\n",
    "    ## Implement the actual gradient descent using the previously implemented functions\n",
    "    for ep in range(num_epochs):\n",
    "        # Compute prediction using current weights\n",
    "        preds = ???\n",
    "        loss, grad_vec = ???\n",
    "        w = ???\n",
    "\n",
    "        # print(w.shape)\n",
    "        # print(grad_vec.shape)\n",
    "        losses.append(loss)\n",
    "        grad_vecs.append(grad_vec)\n",
    "        \n",
    "        if logging:\n",
    "            print(f'Ep {ep+1:2d} | Loss: {loss:.3f}')\n",
    "\n",
    "    return w, losses, grad_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1689299248231,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "qJIOAp7oDA25",
    "outputId": "231c69d7-c63e-4a5d-d0af-eccf706d1544"
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Do not change these settings!\n",
    "# Set a random seed\n",
    "np.random.seed(12345)\n",
    "# ================================================\n",
    "\n",
    "## Run the function on the training set\n",
    "\n",
    "## Start from a random initialisation\n",
    "w_init = np.random.randn(X_train.shape[1],1)\n",
    "\n",
    "# Obtain the final weights via gradient descent\n",
    "w_final, _, _ = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATslkgY4DEdp"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## 2.4 Evaluating the trained model <a class=\"anchor\" id=\"evaluate\"></a>\n",
    "\n",
    "After you have obtained your optimised set of parameters $\\boldsymbol{w}^{*}$, let's see how your model performs! </br>\n",
    "\n",
    "To this end, you are going to:\n",
    "- Obtain the predictions (class probabilities) $\\hat{y}_{train}$ for the training data $\\boldsymbol{X}_{train}$ using $\\boldsymbol{w}^{*}$\n",
    "- Obtain the predictions (class probabilities) $\\hat{y}_{test}$ for the test data $\\boldsymbol{X}_{test}$ using $\\boldsymbol{w}^{*}$\n",
    "- Convert these into the actual predicted labels (everything with probability >=0.5 is more likely to be of class 1 and thus gets label '1' assigned ; below gets label '0')\n",
    "- Count how many samples have been correctly classified and compute the percentage (_i.e._, the accuracy in %)\n",
    "- Report your obtained accuracies for both training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1689299249224,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "3v9tMOk9DGg6",
    "outputId": "b5d7d6dc-f237-4d95-e7b9-0adf9c5fa45d"
   },
   "outputs": [],
   "source": [
    "## Evaluate the obtained model on training data and previously unseen test data\n",
    "\n",
    "# Obtain predicted class probabilities for train and test data\n",
    "y_hat_train = ???\n",
    "y_hat_test  = ???\n",
    "\n",
    "# Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
    "c_hat_train = ???\n",
    "c_hat_test = ???\n",
    "\n",
    "# Evaluate the classification accuracy for training and test data\n",
    "acc_train = ???\n",
    "acc_test = ???\n",
    "\n",
    "# Print outputs\n",
    "print(f'Training accuracy: {acc_train:.3f} | Test accuracy: {acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FToQKYFODIl-"
   },
   "source": [
    "If your implementation work correctly and using the provided hyperparameter settings, you should obtain something around:\n",
    "\n",
    "Training accuracy: 0.868 | Test accuracy: 0.840"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1689299249224,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "ycsTtwAODKTV",
    "outputId": "2c7f97c7-75f4-4a74-a473-c2bab2dc41cf"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "What's the difference between the linear regression and the logistic regression in term of application?\n",
    "\n",
    "Answer:\n",
    "    \n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPyx6JjSIeYU"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Task 3 - Analysing convergence and accuracy <a class=\"anchor\" id=\"analyse-convergence-and-accuracy\"></a>\n",
    "    \n",
    "In this section, your task involves examining logistic regression models using various hyper-parameter configurations and assessing their convergence behavior.  </br>\n",
    "\n",
    "## Improving the accuracy <a class=\"anchor\" id=\"improve-accuracy\"></a>\n",
    "Our previous choice of hyperparameters might not be the best possible one (or even close to it). </br>\n",
    "Can you achieve a **better test accuracy** by changing the hyperparameters from the previous task? </br>\n",
    "Try to improve upon the standard choice by varying the learning rate `lr`. At the meantime, we are going to take a closer look at how gradient descent 'progresses' for different choices of learning rate. Report your choice and best results below!\n",
    "\n",
    "Given the provided set of learning rates _lrs_, run your implemented gradient descent method and plot the obtained loss values over the number of iterations for each learning rate.  </br>\n",
    "Additionally save the training and test accuracies achieved for each learning rate. </br>\n",
    "You can re-use/copy-and-paste your code from above, or define it as a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Learning Objective \n",
    "The aim of this task is to enhance learners' ability to fine-tune hyperparameters for logistic regression models, interpret convergence trends, and make informed decisions about model accuracy and potential overfitting on the dataset. By the end of this lab task, the you will be able to:\n",
    "1. Analyze and compare the test accuracy of the logistic regression model with different hyperparameter settings.\n",
    "2. Evaluate the training convergence of the logistic regression model during the training process and identify any general trends in the convergence behavior.\n",
    "3. Assess whether the model shows signs of overfitting on the given dataset by analyzing the training vs. test accuracy.\n",
    "4. Describe their observations regarding the relationship between the choice of learning rates, convergence results, and accuracy on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1689299249225,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "UwgmDkQUIeYV"
   },
   "outputs": [],
   "source": [
    "## Provided list of learning rates to train on:\n",
    "lrs = [0.05, 0.1, 1.0, 5.0]\n",
    "# Max number of iterations for GD algorithm to run\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1689299249225,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "N5ZpdnwRDeV-",
    "outputId": "1563bcd2-58f4-43a2-e0a2-c57619b3f3b7"
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Do not change these settings!\n",
    "# Set a random seed\n",
    "np.random.seed(12345)\n",
    "# ================================================\n",
    "\n",
    "## Run gradient descent for all learning rates, and plot results\n",
    "fig = plt.subplots()\n",
    "x_vals = range(num_epochs)\n",
    "w_finals = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    ## Start from a random initialisation\n",
    "    w_init = np.random.randn(X_train.shape[1],1)\n",
    "\n",
    "    # Obtain the final weights via gradient descent\n",
    "    w_final, losses, _ = ???\n",
    "    plt.plot(x_vals, losses)\n",
    "    w_finals[f'lr={lr}'] = w_final\n",
    "\n",
    "# Add legend, title and label the axes\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1689299249226,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "qlo5lnP4DhbF",
    "outputId": "04b3952f-8b8a-4a3f-eda6-ef198d8ed36e"
   },
   "outputs": [],
   "source": [
    "# Evaluating the stored parameter sets to retrieve train and test accuracies\n",
    "def evaluate(X,y,w):\n",
    "    # Obtain predicted class probabilities\n",
    "    y_hat = ???\n",
    "\n",
    "    # Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
    "    c_hat = ???\n",
    "\n",
    "    # Evaluate the classification accuracy for training and test data\n",
    "    acc = ???\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print(' >>> Training accuracies for different learning rates: <<<')\n",
    "for k,v in w_finals.items():\n",
    "    print(f'{k}: \\t {round(evaluate(X_train, y_train, v),3)}')\n",
    "\n",
    "print('\\n >>> Test accuracies for different learning rates: <<<')\n",
    "for k,v in w_finals.items():\n",
    "    print(f'{k}: \\t {round(evaluate(X_test, y_test, v), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1689299249226,
     "user": {
      "displayName": "Haoyang Jiang",
      "userId": "08128655751217950193"
     },
     "user_tz": -600
    },
    "id": "Cc0rMVEDDo5O",
    "outputId": "0e8ac454-20a5-4387-d716-993d3db30625"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Describe & Explain\n",
    "\n",
    "Answer the following questions and elaborate on your observations:\n",
    "\n",
    "1 What do you observe? Are there general trends in convergence visible, and are they good or bad?\n",
    "\n",
    "2 What do you think would be the best choice from the set of provided learning rates, and why?\n",
    "\n",
    "### Answer\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp_LLewYXRnf"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "# <p style=\"text-align: center;\">The End</p>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_pHwx0_pExd-",
    "BCA0XOeFGJCt",
    "6K9lmwbBgNXy"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
